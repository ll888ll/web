name: Bind Deploy

# Despliega servidores autoritativos BIND9 (master/slave) tras validar configuraciones.
on:
  push:
    branches: [main, dev]

permissions:
  contents: read
  id-token: write

# Variables globales alimentadas desde GitHub Secrets.
env:
  DOMAIN: ${{ secrets.BIND_DOMAIN != '' && secrets.BIND_DOMAIN || 'croody.app' }}
  MASTER_IP: ${{ secrets.BIND_MASTER_PRIVATE_IP != '' && secrets.BIND_MASTER_PRIVATE_IP || '127.0.0.1' }}
  SLAVE_IP: ${{ secrets.BIND_SLAVE_PRIVATE_IP != '' && secrets.BIND_SLAVE_PRIVATE_IP || '127.0.0.1' }}
  TSIG_KEY_NAME: ${{ secrets.BIND_TSIG_KEY_NAME != '' && secrets.BIND_TSIG_KEY_NAME || 'lint-key' }}
  TSIG_KEY_SECRET: ${{ secrets.BIND_TSIG_KEY_SECRET != '' && secrets.BIND_TSIG_KEY_SECRET || 'ZmFrZQ==' }}
  ECR_REGISTRY: ${{ secrets.AWS_ECR_REGISTRY != '' && secrets.AWS_ECR_REGISTRY || 'ecr.local' }}
  AWS_REGION: ${{ secrets.AWS_REGION != '' && secrets.AWS_REGION || 'us-east-1' }}
  MASTER_REPO: ${{ secrets.BIND_MASTER_ECR_REPO != '' && secrets.BIND_MASTER_ECR_REPO || 'bind-master' }}
  SLAVE_REPO: ${{ secrets.BIND_SLAVE_ECR_REPO != '' && secrets.BIND_SLAVE_ECR_REPO || 'bind-slave' }}
  NS1_FQDN: ns1.${{ secrets.BIND_DOMAIN != '' && secrets.BIND_DOMAIN || 'croody.app' }}.
  NS2_FQDN: ns2.${{ secrets.BIND_DOMAIN != '' && secrets.BIND_DOMAIN || 'croody.app' }}.

jobs:
  build-deploy:
    runs-on: ubuntu-latest
    steps:
      # 1) Checkout del repositorio con la infraestructura DNS.
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Validate required secrets
        run: |
          # Log which values are using defaults vs secrets (for debugging)
          echo "Configuration status:"
          echo "  DOMAIN: ${DOMAIN} (default: croody.app)"
          echo "  MASTER_IP: ${MASTER_IP}"
          echo "  SLAVE_IP: ${SLAVE_IP}"
          echo "  TSIG_KEY_NAME: ${TSIG_KEY_NAME}"
          echo "  ECR_REGISTRY: ${ECR_REGISTRY}"
          echo "  MASTER_REPO: ${MASTER_REPO}"
          echo "  SLAVE_REPO: ${SLAVE_REPO}"

          # All variables have defaults in the env block, so they should never be empty
          # Only warn if using placeholder defaults that won't work for actual deployment
          if [[ "${MASTER_IP}" == "127.0.0.1" || "${SLAVE_IP}" == "127.0.0.1" ]]; then
            echo "::warning::Using placeholder IPs (127.0.0.1). Set BIND_MASTER_PRIVATE_IP and BIND_SLAVE_PRIVATE_IP secrets for actual deployment."
          fi

          if [[ "${ECR_REGISTRY}" == "ecr.local" ]]; then
            echo "::warning::ECR registry not configured. Images will be built but not pushed."
          fi

      - name: Export computed variables
        run: |
          echo "ZONE_FILE=infra/dns/bind-master/zones/${DOMAIN}.db" >> "$GITHUB_ENV"
          echo "MASTER_IMAGE=${ECR_REGISTRY}/${MASTER_REPO}:${GITHUB_SHA}" >> "$GITHUB_ENV"
          echo "SLAVE_IMAGE=${ECR_REGISTRY}/${SLAVE_REPO}:${GITHUB_SHA}" >> "$GITHUB_ENV"

      - name: Check ECR configuration
        id: ecr-check
        run: |
          ECR_REGISTRY="${{ secrets.AWS_ECR_REGISTRY }}"
          AWS_KEY="${{ secrets.AWS_ACCESS_KEY_ID }}"
          AWS_SECRET="${{ secrets.AWS_SECRET_ACCESS_KEY }}"

          if [[ -n "$ECR_REGISTRY" && -n "$AWS_KEY" && -n "$AWS_SECRET" ]]; then
            # Validate ECR registry URL format: <account-id>.dkr.ecr.<region>.amazonaws.com
            if [[ "$ECR_REGISTRY" =~ ^[0-9]+\.dkr\.ecr\.[a-z0-9-]+\.amazonaws\.com$ ]]; then
              echo "ecr_configured=true" >> "$GITHUB_OUTPUT"
              echo "ECR registry URL format is valid: $ECR_REGISTRY"
            else
              echo "ecr_configured=false" >> "$GITHUB_OUTPUT"
              echo "::error::ECR registry URL format is invalid: $ECR_REGISTRY"
              echo "::error::Expected format: <account-id>.dkr.ecr.<region>.amazonaws.com"
              echo "::warning::Skipping ECR push due to invalid registry URL format. This may cause TLS certificate errors."
            fi
          else
            echo "ecr_configured=false" >> "$GITHUB_OUTPUT"
            echo "::warning::ECR not configured - skipping image push. Set AWS_ECR_REGISTRY, AWS_ACCESS_KEY_ID, and AWS_SECRET_ACCESS_KEY secrets to enable."
          fi

      # 2) Ensure zone file exists - generate from template if missing.
      - name: Ensure zone file exists
        run: |
          if [[ ! -f "${ZONE_FILE}" ]]; then
            echo "Zone file ${ZONE_FILE} not found. Generating from template..."

            # Create zones directory if it doesn't exist
            mkdir -p "$(dirname "${ZONE_FILE}")"

            # Generate zone file from template
            ZONE_TEMPLATE="infra/dns/templates/zone.db.tpl"
            if [[ ! -f "${ZONE_TEMPLATE}" ]]; then
              echo "::error::Zone template ${ZONE_TEMPLATE} not found."
              exit 1
            fi

            # Set required variables for template substitution
            export DNS_DOMAIN="${DOMAIN}"
            export NS1_FQDN="ns1.${DOMAIN}."
            export NS2_FQDN="ns2.${DOMAIN}."
            export ZONE_DEFAULT_TTL=3600
            export SOA_EMAIL_DNS="hostmaster.${DOMAIN//@/.}"
            export SERIAL="$(date +%Y%m%d%H)"
            export GLUE_RECORDS="ns1    3600 IN A ${MASTER_IP}
          ns2    3600 IN A ${SLAVE_IP}"
            export A_RECORDS="; Add your A records here"
            export AAAA_RECORDS="; Add your AAAA records here"
            export CNAME_RECORDS="; Add your CNAME records here"
            export MX_RECORDS="; Add your MX records here"
            export TXT_RECORDS="; Add your TXT/SPF records here"

            envsubst < "${ZONE_TEMPLATE}" > "${ZONE_FILE}"
            echo "Generated zone file: ${ZONE_FILE}"
            cat "${ZONE_FILE}"
          else
            echo "Zone file found: ${ZONE_FILE}"
          fi

      # 3) Configure AWS credentials and login to ECR (if properly configured).
      - name: Configure AWS credentials
        if: steps.ecr-check.outputs.ecr_configured == 'true'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Login to Amazon ECR
        if: steps.ecr-check.outputs.ecr_configured == 'true'
        uses: aws-actions/amazon-ecr-login@v2
        with:
          registry-type: private
          mask-password: true

      # 4) Construcción de la imagen maestro.
      - name: Build master image
        run: |
          docker build \
            -f infra/dns/Dockerfile \
            -t "$MASTER_IMAGE" \
            infra/dns

      # 5) Construcción de la imagen esclavo (reutiliza el mismo Dockerfile).
      - name: Build slave image
        run: |
          docker build \
            -f infra/dns/Dockerfile \
            -t "$SLAVE_IMAGE" \
            infra/dns

      # 6) Validar configuraciones BIND del maestro: named.conf + zona.
      - name: Validate BIND configs (master)
        run: |
          docker run --rm \
            -e DNS_ROLE=master \
            -e DNS_DOMAIN="${DOMAIN}" \
            -e TSIG_KEY_NAME="${TSIG_KEY_NAME}" \
            -e TSIG_KEY_SECRET="${TSIG_KEY_SECRET}" \
            -e NS1_FQDN="${NS1_FQDN}" \
            -e NS2_FQDN="${NS2_FQDN}" \
            -e ALLOW_QUERY="any;" \
            -e ALLOW_TRANSFER="${SLAVE_IP};" \
            -e NOTIFY_TARGETS="${SLAVE_IP};" \
            -e MASTER_IP="${MASTER_IP}" \
            -v "${PWD}/infra/dns/bind-master/zones:/zones" \
            "$MASTER_IMAGE" \
            named-checkconf /etc/bind/named.conf

          docker run --rm \
            -e DNS_ROLE=master \
            -e DNS_DOMAIN="${DOMAIN}" \
            -e TSIG_KEY_NAME="${TSIG_KEY_NAME}" \
            -e TSIG_KEY_SECRET="${TSIG_KEY_SECRET}" \
            -e NS1_FQDN="${NS1_FQDN}" \
            -e NS2_FQDN="${NS2_FQDN}" \
            -v "${PWD}/infra/dns/bind-master/zones:/zones" \
            "$MASTER_IMAGE" \
            named-checkzone "${DOMAIN}" "/zones/${DOMAIN}.db"

      # 7) Validar configuración del esclavo (named.conf); la zona se replica vía AXFR.
      - name: Validate BIND configs (slave)
        run: |
          docker run --rm \
            -e DNS_ROLE=slave \
            -e DNS_DOMAIN="${DOMAIN}" \
            -e TSIG_KEY_NAME="${TSIG_KEY_NAME}" \
            -e TSIG_KEY_SECRET="${TSIG_KEY_SECRET}" \
            -e NS1_FQDN="${NS1_FQDN}" \
            -e NS2_FQDN="${NS2_FQDN}" \
            -e MASTER_IP="${MASTER_IP}" \
            -v "${PWD}/infra/dns/bind-slave/zones:/zones" \
            "$SLAVE_IMAGE" \
            named-checkconf /etc/bind/named.conf

      # 8) Publicar imágenes en ECR si corresponde (opcional).
      - name: Push master image to ECR
        if: steps.ecr-check.outputs.ecr_configured == 'true'
        run: docker push "$MASTER_IMAGE"

      - name: Push slave image to ECR
        if: steps.ecr-check.outputs.ecr_configured == 'true'
        run: docker push "$SLAVE_IMAGE"

      # 9) Despliegue remoto en el maestro: pull de imagen, recrear contenedor, recargar zona vía rndc.
      - name: Deploy to master host via SSH (pull from ECR)
        if: steps.ecr-check.outputs.ecr_configured == 'true'
        uses: appleboy/ssh-action@v1.1.0
        with:
          host: ${{ secrets.BIND_MASTER_HOST }}
          username: ${{ secrets.BIND_SSH_USER }}
          key: ${{ secrets.BIND_SSH_KEY }}
          port: ${{ secrets.BIND_SSH_PORT || '22' }}
          script_stop: true
          envs: MASTER_IMAGE,DOMAIN,TSIG_KEY_NAME,TSIG_KEY_SECRET,MASTER_IP,SLAVE_IP,NS1_FQDN,NS2_FQDN
          script: |
            set -euo pipefail
            docker pull "$MASTER_IMAGE"
            docker stop bind-master || true
            docker rm bind-master || true
            docker run -d \
              --name bind-master \
              --restart unless-stopped \
              -p 53:53/tcp -p 53:53/udp \
              -v /etc/bind/master/zones:/zones \
              -v /etc/bind/master/keys:/keys \
              -e DNS_ROLE=master \
              -e DNS_DOMAIN=${DOMAIN} \
              -e TSIG_KEY_NAME=${TSIG_KEY_NAME} \
              -e TSIG_KEY_SECRET=${TSIG_KEY_SECRET} \
              -e NS1_FQDN=${NS1_FQDN} \
              -e NS2_FQDN=${NS2_FQDN} \
              -e MASTER_IP=${MASTER_IP} \
              -e ALLOW_QUERY="any;" \
              -e ALLOW_TRANSFER="${SLAVE_IP};" \
              -e NOTIFY_TARGETS="${SLAVE_IP};" \
              "$MASTER_IMAGE"
            docker exec bind-master rndc reload || true

      # 10) Despliegue remoto del esclavo y recarga de zona.
      - name: Deploy to slave host via SSH (pull from ECR)
        if: steps.ecr-check.outputs.ecr_configured == 'true'
        uses: appleboy/ssh-action@v1.1.0
        with:
          host: ${{ secrets.BIND_SLAVE_HOST }}
          username: ${{ secrets.BIND_SSH_USER }}
          key: ${{ secrets.BIND_SSH_KEY }}
          port: ${{ secrets.BIND_SSH_PORT || '22' }}
          script_stop: true
          envs: SLAVE_IMAGE,DOMAIN,TSIG_KEY_NAME,TSIG_KEY_SECRET,MASTER_IP,SLAVE_IP,NS1_FQDN,NS2_FQDN
          script: |
            set -euo pipefail
            docker pull "$SLAVE_IMAGE"
            docker stop bind-slave || true
            docker rm bind-slave || true
            docker run -d \
              --name bind-slave \
              --restart unless-stopped \
              -p 53:53/tcp -p 53:53/udp \
              -v /etc/bind/slave/zones:/zones \
              -v /etc/bind/slave/keys:/keys \
              -e DNS_ROLE=slave \
              -e DNS_DOMAIN=${DOMAIN} \
              -e TSIG_KEY_NAME=${TSIG_KEY_NAME} \
              -e TSIG_KEY_SECRET=${TSIG_KEY_SECRET} \
              -e NS1_FQDN=${NS1_FQDN} \
              -e NS2_FQDN=${NS2_FQDN} \
              -e MASTER_IP=${MASTER_IP} \
              "$SLAVE_IMAGE"
            docker exec bind-slave rndc reload || true
